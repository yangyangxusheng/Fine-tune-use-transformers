{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Fine-tune-roberta to do classification","provenance":[],"collapsed_sections":[],"mount_file_id":"1aJo8G95qiIpeFSKBv_cTwFj1MO-cQPF5","authorship_tag":"ABX9TyOFIVBl4FmmOKcBl0HkcdBn"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"4052e08fbfc74d54afff37639c4320c2":{"model_module":"@jupyter-widgets/controls","model_name":"VBoxModel","model_module_version":"1.5.0","state":{"_view_name":"VBoxView","_dom_classes":[],"_model_name":"VBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_9922c0481b86474b8c4feaaaffe02336","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4a0fbe938c384e21a02e8b09c214500b","IPY_MODEL_e05faac6ed094aab813526f11798240d","IPY_MODEL_e88f02e8913c4f1089801a88473c497a"]}},"9922c0481b86474b8c4feaaaffe02336":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"column","width":"50%","min_width":null,"border":null,"align_items":"center","bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"flex","left":null}},"4a0fbe938c384e21a02e8b09c214500b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3cdff25cb126490da1c989a490900c65","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"<center>\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.svg alt='Hugging Face'>\n<br>\n<b>The AI community building the future</b>\n<br>\nImmediately click login after typing your password or it might be stored in plain text in this notebook file.\n</center>","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_593b4d4093c941138244f6381f3da142"}},"e05faac6ed094aab813526f11798240d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d56f0e25cb2c4428b56956d8689cd76d","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_4651017a96664b01865096a4b253b90d","IPY_MODEL_005153b290da43648128465299547dbc"]}},"e88f02e8913c4f1089801a88473c497a":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_view_name":"ButtonView","style":"IPY_MODEL_695cf4eceeaa416396dc0c20a21062e5","_dom_classes":[],"description":"Login","_model_name":"ButtonModel","button_style":"","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","tooltip":"","_view_count":null,"disabled":false,"_view_module_version":"1.5.0","layout":"IPY_MODEL_829c12b0a09c44b1972aba27c4fbd139","_model_module":"@jupyter-widgets/controls","icon":""}},"3cdff25cb126490da1c989a490900c65":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"593b4d4093c941138244f6381f3da142":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"d56f0e25cb2c4428b56956d8689cd76d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"4651017a96664b01865096a4b253b90d":{"model_module":"@jupyter-widgets/controls","model_name":"TextModel","model_module_version":"1.5.0","state":{"_view_name":"TextView","style":"IPY_MODEL_bd0f01d6032f411cb84bf13d4871e95d","_dom_classes":[],"description":"Username:","_model_name":"TextModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"760565201@qq.com","_view_count":null,"disabled":false,"_view_module_version":"1.5.0","continuous_update":true,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c2d460aa746c4644b25d22957bd4f974"}},"005153b290da43648128465299547dbc":{"model_module":"@jupyter-widgets/controls","model_name":"PasswordModel","model_module_version":"1.5.0","state":{"_view_name":"PasswordView","style":"IPY_MODEL_5b4fa8f66fc84530bdde411eda19e0e0","_dom_classes":[],"description":"Password:","_model_name":"PasswordModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"","_view_count":null,"disabled":false,"_view_module_version":"1.5.0","continuous_update":true,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d551483365774f81b9470d331c0dc79a"}},"695cf4eceeaa416396dc0c20a21062e5":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ButtonStyleModel","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"button_color":null,"font_weight":"","_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"829c12b0a09c44b1972aba27c4fbd139":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"bd0f01d6032f411cb84bf13d4871e95d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c2d460aa746c4644b25d22957bd4f974":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5b4fa8f66fc84530bdde411eda19e0e0":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d551483365774f81b9470d331c0dc79a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"df017eff0af3461abc178bdc377289e2":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_bd4a99ecdd9a493287030f32dd26aa82","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_0b418438981349a39aa149ca5dace928","IPY_MODEL_0fda3c11db304870bb1f849823ff38a1","IPY_MODEL_1b58ef7108b949bbbb97cf019956d425"]}},"bd4a99ecdd9a493287030f32dd26aa82":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"0b418438981349a39aa149ca5dace928":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_feb9991983e9472c89fcda1ea413e0b1","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Upload file pytorch_model.bin: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_d475fc618aad403580dde914927d228d"}},"0fda3c11db304870bb1f849823ff38a1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e3a6f7e66d44373b732fa26dd353f96","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":498674093,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":498674093,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_224d6b054cee4364ba56e258ddd3d2c1"}},"1b58ef7108b949bbbb97cf019956d425":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ece08140e76042a1842ba2144517855f","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"‚Äã","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 476M/476M [06:10&lt;00:00, 1.00MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5fa125b5574487ebc467863ea67dd47"}},"feb9991983e9472c89fcda1ea413e0b1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"d475fc618aad403580dde914927d228d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e3a6f7e66d44373b732fa26dd353f96":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"224d6b054cee4364ba56e258ddd3d2c1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"ece08140e76042a1842ba2144517855f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e5fa125b5574487ebc467863ea67dd47":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"cxveXH-ZrrfU"},"source":["ÂÖàÁúãÂàÜÈÖçÂà∞‰∫Ü‰ªÄ‰πàGPU\n","\n","First we see what gpu we have"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5hqierkIYVgd","executionInfo":{"status":"ok","timestamp":1636461537812,"user_tz":-480,"elapsed":567,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"d716531b-afdf-4637-fbcb-1357e9149974"},"source":["!nvidia-smi"],"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Tue Nov  9 12:38:56 2021       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 495.44       Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   46C    P0    37W / 250W |   6315MiB / 16280MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rU_5YdoBXZDT","executionInfo":{"status":"ok","timestamp":1636461549743,"user_tz":-480,"elapsed":8336,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"efddfadd-be9f-4caf-f963-cde87dc01f3e"},"source":["!pip install transformers\n","!pip install datasets\n","!pip install sklearn"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.12.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.1.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.7/dist-packages (1.15.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.1.2)\n","Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2021.11.0)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.62.3)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.7/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.8.1)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets) (3.8.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n","Requirement already satisfied: pyarrow!=4.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.3.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.5.30)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (0.13.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.2.0)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.7.2)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.7)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (4.0.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (5.2.0)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (1.2.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.6.0)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n","Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n","Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FAevguZKnUCC","executionInfo":{"status":"ok","timestamp":1636461551943,"user_tz":-480,"elapsed":3,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"24b9b177-72ed-40c8-9e08-2fa8a215c75e"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"LtzS-Ugxte3_"},"source":["Â∞ÜÂæÆË∞ÉÂêéÁöÑÊ®°Âûã‰∏ä‰º†Âà∞huggingfaceÈáåÈù¢"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":237,"referenced_widgets":["4052e08fbfc74d54afff37639c4320c2","9922c0481b86474b8c4feaaaffe02336","4a0fbe938c384e21a02e8b09c214500b","e05faac6ed094aab813526f11798240d","e88f02e8913c4f1089801a88473c497a","3cdff25cb126490da1c989a490900c65","593b4d4093c941138244f6381f3da142","d56f0e25cb2c4428b56956d8689cd76d","4651017a96664b01865096a4b253b90d","005153b290da43648128465299547dbc","695cf4eceeaa416396dc0c20a21062e5","829c12b0a09c44b1972aba27c4fbd139","bd0f01d6032f411cb84bf13d4871e95d","c2d460aa746c4644b25d22957bd4f974","5b4fa8f66fc84530bdde411eda19e0e0","d551483365774f81b9470d331c0dc79a"]},"id":"CxYrfm4ZnVDt","executionInfo":{"status":"ok","timestamp":1636461555976,"user_tz":-480,"elapsed":516,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"85603d85-2c18-4d8d-c893-24f180ada070"},"source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-crendential store but this isn't the helper defined on your machine.\n","You will have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal to set it as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkDQgjMwnfgL","executionInfo":{"status":"ok","timestamp":1636461593117,"user_tz":-480,"elapsed":2705,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"844cf2c1-9f42-416d-dfb1-fd6f9e2befe8"},"source":["!apt install git-lfs"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","git-lfs is already the newest version (2.3.4-1).\n","0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"]}]},{"cell_type":"code","metadata":{"id":"DjNTFRp_X6Dj","executionInfo":{"status":"ok","timestamp":1636461597979,"user_tz":-480,"elapsed":2,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}}},"source":["import pandas as pd\n","from transformers import AutoTokenizer\n","from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n","import numpy as np\n","from datasets import load_metric\n","from sklearn.model_selection import train_test_split"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0YGLxsMwY0ee"},"source":["data preprocess Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜ"]},{"cell_type":"markdown","metadata":{"id":"sn3FO-lytnTL"},"source":["ÂÖàÊää‰∏§‰∏™jsonÈáåÈù¢ÁöÑÂÜÖÂÆπËØªÂèñÊàêpandasÁöÑÊ†ºÂºèÔºåÂÜçÊãºÊé•Êàê‰∏Ä‰∏™Â§ßÊï∞ÊçÆÈõÜ"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WIZQpDAfPYIZ","executionInfo":{"status":"ok","timestamp":1636461638045,"user_tz":-480,"elapsed":409,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"af6e4137-02a4-423f-944a-169b628e496e"},"source":["import pandas as pd\n","import os\n","data = pd.read_json('Sarcasm_Headlines_Dataset.json', lines=True)\n","data_v2 = pd.read_json('Sarcasm_Headlines_Dataset_v2.json', lines=True)\n","all_data = pd.concat([data,data_v2] , ignore_index = True , axis =0)\n","print(all_data.shape)"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["(55328, 3)\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c82CpryUSc4n","executionInfo":{"status":"ok","timestamp":1636461640197,"user_tz":-480,"elapsed":3,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"975549a3-a21b-49a1-fb61-29e611455763"},"source":["all_data.info"],"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method DataFrame.info of                                             article_link  ... is_sarcastic\n","0      https://www.huffingtonpost.com/entry/versace-b...  ...            0\n","1      https://www.huffingtonpost.com/entry/roseanne-...  ...            0\n","2      https://local.theonion.com/mom-starting-to-fea...  ...            1\n","3      https://politics.theonion.com/boehner-just-wan...  ...            1\n","4      https://www.huffingtonpost.com/entry/jk-rowlin...  ...            0\n","...                                                  ...  ...          ...\n","55323  https://www.theonion.com/jews-to-celebrate-ros...  ...            1\n","55324  https://local.theonion.com/internal-affairs-in...  ...            1\n","55325  https://www.huffingtonpost.com/entry/andrew-ah...  ...            0\n","55326  https://www.theonion.com/mars-probe-destroyed-...  ...            1\n","55327  https://www.theonion.com/dad-clarifies-this-no...  ...            1\n","\n","[55328 rows x 3 columns]>"]},"metadata":{},"execution_count":27}]},{"cell_type":"markdown","metadata":{"id":"4v8yExw5twvK"},"source":["Âª∫‰∏Ä‰∏™Á©∫ÁöÑDataFrameÁî®Êù•‰øùÂ≠òÊï∞ÊçÆ"]},{"cell_type":"code","metadata":{"id":"aBZW0OkAZoQ3","executionInfo":{"status":"ok","timestamp":1636461673898,"user_tz":-480,"elapsed":2,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}}},"source":["df = pd.DataFrame()\n","df['text'] = all_data['headline']\n","df['label'] = all_data['is_sarcastic']"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"usr1Zr1yt1yn"},"source":["ÂàíÂàÜÊï∞ÊçÆÈõÜ"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ylHCoumAaTTg","executionInfo":{"status":"ok","timestamp":1636461675963,"user_tz":-480,"elapsed":3,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"d598bb77-63d7-41e5-a16d-a8d0cc4534b0"},"source":["from sklearn.model_selection import train_test_split\n","\n","df_sample = df.sample(frac = 1)\n","\n","train_df,test_df = train_test_split(df_sample,test_size = 0.3)\n","\n","print(train_df.shape)\n","print(test_df.shape)"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["(38729, 2)\n","(16599, 2)\n"]}]},{"cell_type":"code","metadata":{"id":"NVQoWLoJasEJ","executionInfo":{"status":"ok","timestamp":1636461678667,"user_tz":-480,"elapsed":400,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}}},"source":["train_df2 = pd.DataFrame({\n","    'text': train_df['text'].replace(r'\\n', ' ', regex=True),\n","    'label': train_df['label']\n","})\n","\n","test_df2 = pd.DataFrame({\n","    'text': test_df['text'].replace(r'\\n', ' ', regex=True),\n","    'label': test_df['label']\n","})"],"execution_count":34,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fzj3jABSt8DR"},"source":["ËΩ¨ÊàêÂàóË°®ÁöÑÊ†ºÂºè"]},{"cell_type":"code","metadata":{"id":"aNwBEbogeCcN","executionInfo":{"status":"ok","timestamp":1636461686444,"user_tz":-480,"elapsed":615,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}}},"source":["train_texts, val_texts, train_labels, val_labels = train_test_split(train_df2['text'], train_df2['label'], test_size=.1)\n","test_texts = test_df2['text']\n","test_labels = test_df2['label']\n","train_texts = list(train_texts)\n","val_texts = list(val_texts)\n","test_texts = list(test_texts)\n","train_labels = list(train_labels)\n","val_labels = list(val_labels)\n","test_labels = list(test_labels)"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CDBRD-gMX-8O","executionInfo":{"status":"ok","timestamp":1636461707056,"user_tz":-480,"elapsed":7818,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"a27a7f22-725b-473a-9a5f-febd1ce3239e"},"source":["tokenizer = AutoTokenizer.from_pretrained(\"roberta-base\")\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True)\n","test_encodings = tokenizer(test_texts, truncation=True, padding=True)"],"execution_count":36,"outputs":[{"output_type":"stream","name":"stderr","text":["Could not locate the tokenizer configuration file, will try to use the model config instead.\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.3\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading file https://huggingface.co/roberta-base/resolve/main/vocab.json from cache at /root/.cache/huggingface/transformers/d3ccdbfeb9aaa747ef20432d4976c32ee3fa69663b379deb253ccfce2bb1fdc5.d67d6b367eb24ab43b08ad55e014cf254076934f71d832bbab9ad35644a375ab\n","loading file https://huggingface.co/roberta-base/resolve/main/merges.txt from cache at /root/.cache/huggingface/transformers/cafdecc90fcab17011e12ac813dd574b4b3fea39da6dd817813efa010262ff3f.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n","loading file https://huggingface.co/roberta-base/resolve/main/tokenizer.json from cache at /root/.cache/huggingface/transformers/d53fc0fa09b8342651efd4073d75e19617b3e51287c2a535becda5808a8db287.fc9576039592f026ad76a1c231b89aee8668488c671dfbe6616bab2ed298d730\n","loading file https://huggingface.co/roberta-base/resolve/main/added_tokens.json from cache at None\n","loading file https://huggingface.co/roberta-base/resolve/main/special_tokens_map.json from cache at None\n","loading file https://huggingface.co/roberta-base/resolve/main/tokenizer_config.json from cache at None\n","loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.3\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"9qrxEkFluAND"},"source":["Âà∂‰ΩúÊï∞ÊçÆÈõÜ"]},{"cell_type":"code","metadata":{"id":"OhcDWM4kgZhR","executionInfo":{"status":"ok","timestamp":1636461717312,"user_tz":-480,"elapsed":702,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}}},"source":["import torch\n","\n","class SarcasmDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.tensor(self.labels[idx])\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","train_dataset = SarcasmDataset(train_encodings, train_labels)\n","val_dataset = SarcasmDataset(val_encodings, val_labels)\n","test_dataset = SarcasmDataset(test_encodings, test_labels)"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cH-XGcyokjAc"},"source":["define metric"]},{"cell_type":"code","metadata":{"id":"ffgFeM98kmSF","executionInfo":{"status":"ok","timestamp":1636461720708,"user_tz":-480,"elapsed":1193,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}}},"source":["import numpy as np\n","from datasets import load_metric\n","\n","metric = load_metric(\"accuracy\", \"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    predictions = np.argmax(logits, axis=-1)\n","    return metric.compute(predictions=predictions, references=labels)"],"execution_count":39,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NGUcoSodXdpR"},"source":["fine-tune roberta ÂæÆË∞ÉrobertaÊ®°Âûã\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"kuNFQRbZYBax","executionInfo":{"status":"ok","timestamp":1636463293910,"user_tz":-480,"elapsed":1365436,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"60857adb-847d-43c7-86ee-7a3cd50f97ef"},"source":["model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n","training_args = TrainingArguments(\n","    \"roberta-scarcasm-discriminator\",          # output directory\n","    evaluation_strategy = \"epoch\",\n","    num_train_epochs=4,              # total number of training epochs\n","    per_device_train_batch_size=16,  # batch size per device during training\n","    per_device_eval_batch_size=16,   # batch size for evaluation\n","    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n","    weight_decay=0.01,               # strength of weight decay\n","    save_total_limit=3,\n","    logging_dir='./logs',            # directory for storing logs\n","    logging_steps=10,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,                         # the instantiated ü§ó Transformers model to be trained\n","    args=training_args,                  # training arguments, defined above\n","    train_dataset=train_dataset,         # training dataset\n","    eval_dataset=val_dataset,             # evaluation dataset\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()\n"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stderr","text":["loading configuration file https://huggingface.co/roberta-base/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/733bade19e5f0ce98e6531021dd5180994bb2f7b8bd7e80c7968805834ba351e.35205c6cfc956461d8515139f0f8dd5d207a2f336c0c3a83b4bc8dca3518e37b\n","Model config RobertaConfig {\n","  \"architectures\": [\n","    \"RobertaForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"classifier_dropout\": null,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"roberta\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"position_embedding_type\": \"absolute\",\n","  \"transformers_version\": \"4.12.3\",\n","  \"type_vocab_size\": 1,\n","  \"use_cache\": true,\n","  \"vocab_size\": 50265\n","}\n","\n","loading weights file https://huggingface.co/roberta-base/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/51ba668f7ff34e7cdfa9561e8361747738113878850a7d717dbc69de8683aaad.c7efaa30a0d80b2958b876969faa180e485944a849deee4ad482332de65365a7\n","Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","PyTorch: setting up devices\n","The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n","Cloning https://huggingface.co/XSY/roberta-scarcasm-discriminator into local empty directory.\n","***** Running training *****\n","  Num examples = 34856\n","  Num Epochs = 4\n","  Instantaneous batch size per device = 16\n","  Total train batch size (w. parallel, distributed & accumulation) = 16\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 8716\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='8716' max='8716' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [8716/8716 22:34, Epoch 4/4]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.069600</td>\n","      <td>0.355612</td>\n","      <td>0.914020</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.114000</td>\n","      <td>0.211543</td>\n","      <td>0.941905</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.002300</td>\n","      <td>0.164517</td>\n","      <td>0.966951</td>\n","    </tr>\n","    <tr>\n","      <td>4</td>\n","      <td>0.001200</td>\n","      <td>0.164033</td>\n","      <td>0.970565</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-500/pytorch_model.bin\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-1000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-1000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-1000/pytorch_model.bin\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-1500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-1500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-1500/pytorch_model.bin\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-2000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-2000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-2000/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-500] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3873\n","  Batch size = 16\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-2500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-2500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-2500/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-1000] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-3000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-3000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-3000/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-1500] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-3500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-3500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-3500/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-2000] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-4000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-4000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-4000/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-2500] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3873\n","  Batch size = 16\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-4500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-4500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-4500/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-3000] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-5000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-5000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-5000/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-3500] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-5500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-5500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-5500/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-4000] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-6000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-6000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-6000/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-4500] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-6500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-6500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-6500/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-5000] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3873\n","  Batch size = 16\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-7000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-7000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-7000/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-5500] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-7500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-7500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-7500/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-6000] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-8000\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-8000/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-8000/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-6500] due to args.save_total_limit\n","Saving model checkpoint to roberta-scarcasm-discriminator/checkpoint-8500\n","Configuration saved in roberta-scarcasm-discriminator/checkpoint-8500/config.json\n","Model weights saved in roberta-scarcasm-discriminator/checkpoint-8500/pytorch_model.bin\n","Deleting older checkpoint [roberta-scarcasm-discriminator/checkpoint-7000] due to args.save_total_limit\n","***** Running Evaluation *****\n","  Num examples = 3873\n","  Batch size = 16\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=8716, training_loss=0.14337734355059756, metrics={'train_runtime': 1354.6061, 'train_samples_per_second': 102.926, 'train_steps_per_second': 6.434, 'total_flos': 4872093189864960.0, 'train_loss': 0.14337734355059756, 'epoch': 4.0})"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":324,"referenced_widgets":["df017eff0af3461abc178bdc377289e2","bd4a99ecdd9a493287030f32dd26aa82","0b418438981349a39aa149ca5dace928","0fda3c11db304870bb1f849823ff38a1","1b58ef7108b949bbbb97cf019956d425","feb9991983e9472c89fcda1ea413e0b1","d475fc618aad403580dde914927d228d","7e3a6f7e66d44373b732fa26dd353f96","224d6b054cee4364ba56e258ddd3d2c1","ece08140e76042a1842ba2144517855f","e5fa125b5574487ebc467863ea67dd47"]},"id":"Asv4ZP-AkOBz","executionInfo":{"status":"ok","timestamp":1636463690602,"user_tz":-480,"elapsed":395522,"user":{"displayName":"Aaron Yang","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02392162463703366496"}},"outputId":"5bb14c0a-9506-4720-92b5-f40a93d38f60"},"source":["trainer.push_to_hub(\"XSY/roberta-scarcasm-discriminator\")"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to roberta-scarcasm-discriminator\n","Configuration saved in roberta-scarcasm-discriminator/config.json\n","Model weights saved in roberta-scarcasm-discriminator/pytorch_model.bin\n","Several commits (2) will be pushed upstream.\n","The progress bars may be unreliable.\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"df017eff0af3461abc178bdc377289e2","version_minor":0,"version_major":2},"text/plain":["Upload file pytorch_model.bin:   0%|          | 3.38k/476M [00:00<?, ?B/s]"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["To https://huggingface.co/XSY/roberta-scarcasm-discriminator\n","   fc10289..54051f3  main -> main\n","\n","Dropping the following result as it does not have all the necessary fields:\n","{'task': {'name': 'Text Classification', 'type': 'text-classification'}, 'metrics': [{'name': 'Accuracy', 'type': 'accuracy', 'value': 0.9705654531371031}]}\n","To https://huggingface.co/XSY/roberta-scarcasm-discriminator\n","   54051f3..86c5e88  main -> main\n","\n"]},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'https://huggingface.co/XSY/roberta-scarcasm-discriminator/commit/54051f3d8b2163ef7a7101f88ae13a56310902c4'"]},"metadata":{},"execution_count":42}]}]}